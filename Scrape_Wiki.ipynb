{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scrape_Wiki.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "bHY3F2HMOLHA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Importing packages**"
      ]
    },
    {
      "metadata": {
        "id": "eXLiCM4ALjDy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "# import json\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup as BSoup\n",
        "!conda install -c conda-forge geopy --yes\n",
        "from geopy.geocoders import Nominatim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3eN9yuxxPNcT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Initializing source and destination variables**"
      ]
    },
    {
      "metadata": {
        "id": "SMESwNcvPUC6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BASE_URI = 'https://en.wikipedia.org'\n",
        "page = requests.get(BASE_URI+'/wiki/List_of_neighbourhoods_in_Toronto')\n",
        "soup = BSoup(page.content, 'html.parser')\n",
        "\n",
        "boroughs_list = soup.select(\".mw-parser-output h3\")\n",
        "neighbours_list = soup.select(\".mw-parser-output div table.multicol\")\n",
        "\n",
        "city_info = list()\n",
        "\n",
        "column_names = ['Borough', 'Neighborhood', 'Latitude', 'Longitude'] \n",
        "\n",
        "# instantiate the dataframe\n",
        "neighborhoods = pd.DataFrame(columns=column_names)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hyMv0c4dOdP1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Making use of List of neighbourhoods page to scrape neighbourhoods in Toronto**\n",
        "\n",
        "The latitude and longitude values are also scraped out from Wikipedia itself using these functions:"
      ]
    },
    {
      "metadata": {
        "id": "oYj7TZKwOYVs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def geo_calculator(value):\n",
        "  if len(value) == 4:\n",
        "    decimal = float(value[0]) + (float(value[1])/60) + (float(value[2])/3600)\n",
        "  elif len(value) == 3:\n",
        "    decimal = float(value[0]) + (float(value[1])/60)\n",
        "  elif len(value) == 2:\n",
        "    decimal = float(value[0])\n",
        "  else:\n",
        "    raise ValueError\n",
        "  return decimal if value[-1].strip() in ['N', 'E'] else -decimal\n",
        "\n",
        "def scrape_geodata(url):\n",
        "  page_ = requests.get(url)\n",
        "  soup_ = BSoup(page_.content, 'html.parser')\n",
        "  lat_elem = soup_.select('.geo-default .geo-dms .latitude')\n",
        "  lon_elem = soup_.select('.geo-default .geo-dms .longitude')\n",
        "  if lat_elem:\n",
        "    if 'append' in dir(lat_elem):\n",
        "      lat_elem = lat_elem[0]\n",
        "    lt = re.split(u'[°′″]', lat_elem.get_text())\n",
        "    latitude = geo_calculator(lt)\n",
        "  \n",
        "  else:\n",
        "    lat_elem = soup_.select('.geo-default .geo')[0].get_text()\n",
        "    latitude = lat_elem.split('; ')[0]\n",
        "\n",
        "  if lon_elem:\n",
        "    if 'append' in dir(lon_elem):\n",
        "      lon_elem = lon_elem[0]\n",
        "    ln = re.split(u'[°′″]', lon_elem.get_text())\n",
        "    longitude = geo_calculator(ln)\n",
        "\n",
        "  else:\n",
        "    lon_elem = soup_.select('.geo-default .geo')[0].get_text()\n",
        "    longitude = lon_elem.split('; ')[1]\n",
        "\n",
        "  return latitude, longitude\n",
        "\n",
        "def get_geodata(url):\n",
        "  geolocator = Nominatim()\n",
        "  geo_name = url.split('/')[-1].replace('_', ' ')\n",
        "  location = geolocator.geocode(geo_name)\n",
        "  if location:\n",
        "    return location.latitude, location.longitude\n",
        "  else:\n",
        "    lat, lon = scrape_geodata(BASE_URI + url)\n",
        "    return lat, lon\n",
        "\n",
        "neighbourhood_data = list()\n",
        "for index in range(len(neighbours_list)):\n",
        "  this_borough = boroughs_list[index].find('a').get_text()\n",
        "  neighbours_in_borough = neighbours_list[index].select('td li')\n",
        "  for neighbours in neighbours_in_borough:\n",
        "    neighbour_dict = dict()\n",
        "    neighbour_url = neighbours.find('a').get('href')\n",
        "    neighbour_name = neighbours.find('a').get_text()\n",
        "    neighbour_dict['Borough'] = this_borough.encode('ascii', 'ignore')\n",
        "    neighbour_dict['Neighborhood'] = neighbour_name.encode('ascii', 'ignore')\n",
        "    neighbour_dict['Latitude'], neighbour_dict['Longitude'] = get_geodata(neighbour_url)\n",
        "    neighbourhood_data.append(neighbour_dict)\n",
        "\n",
        "for data in neighbourhood_data:\n",
        "  neighborhoods = neighborhoods.append(data, ignore_index=True)\n",
        "\n",
        "print(neighborhoods.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}